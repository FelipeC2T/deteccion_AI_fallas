{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to disable **GPU**. In my notebook, I cannot use more than one item per mini-batch with my GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "#from tensorflow_addons.losses import SigmoidFocalCrossEntropy  # tfa.losses.SigmoidFocalCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Checkpoints capture the exact value of all parameters (tf.Variable objects)\n",
    "#  used by a model. Checkpoints do not contain any description of the computation\n",
    "#  defined by the model and thus are typically only useful when source code \n",
    "#  that will use the saved parameter values is available.\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # to load custom python code\n",
    "sys.path.append('...\\\\Source')\n",
    "from unet import unet  # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *  # Custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from patchProcessAssemble import Normalize\n",
    "# def Normalize(x):\n",
    "#     # see tf.image.per_image_standardization\n",
    "#     N      = len(x)\n",
    "#     mean   = np.mean(x)\n",
    "#     stddev = np.std(x)\n",
    "#     stddev = np.maximum(stddev, 1.0/np.sqrt(N)) # protect againts division by zero\n",
    "#     return (x - mean) / stddev  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     14,
     18,
     32
    ]
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for keras'\n",
    "    def __init__(self,dpath,fpath,data_IDs, batch_size=1, dim=(128,128,128), \n",
    "                 n_channels=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim   = dim\n",
    "        self.dpath = dpath\n",
    "        self.fpath = fpath\n",
    "        self.batch_size = batch_size\n",
    "        self.data_IDs   = data_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle    = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data_IDs)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        bsize = self.batch_size\n",
    "        indexes = self.indexes[index*bsize:(index+1)*bsize]\n",
    "\n",
    "        # Find list of IDs\n",
    "        data_IDs_temp = [self.data_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(data_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_IDs))\n",
    "        if self.shuffle == True:\n",
    "              np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, data_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        # JLG: data stream is read:\n",
    "        gx  = np.fromfile(self.dpath+str(data_IDs_temp[0])+'.dat',dtype=np.single)\n",
    "        fx  = np.fromfile(self.fpath+str(data_IDs_temp[0])+'.dat',dtype=np.single)\n",
    "        # JLG: data is shaped into a cube\n",
    "        gx = np.reshape(gx,self.dim)\n",
    "        fx = np.reshape(fx,self.dim)\n",
    "        # JLG: data is whitened (normalized):\n",
    "        gx = Normalize(gx)\n",
    "        # JLG: data is transposed to conform with (time,x,y); meaning that the original input is (x,y,time).\n",
    "        gx = np.transpose(gx)\n",
    "        fx = np.transpose(fx)\n",
    "        #in seismic processing, the dimensions of a seismic array is often arranged as\n",
    "        #a[n3][n2][n1] where n1 represnts the vertical dimenstion. This is why we need \n",
    "        #to transpose the array here in python \n",
    "        \n",
    "        # Generate data\n",
    "        \n",
    "        # Local CPU (too slow: use to run for a few epochs for testing. I can use more than one item per mini-batch)\n",
    "        X = np.zeros((2, *self.dim, self.n_channels),dtype=np.single) # JLG: e.g., X is (2,128x128x128,1)\n",
    "        Y = np.zeros((2, *self.dim, self.n_channels),dtype=np.single)\n",
    "        X[0,] = np.reshape(gx, (*self.dim,self.n_channels))  # JLG: the original `image`\n",
    "        Y[0,] = np.reshape(fx, (*self.dim,self.n_channels))  \n",
    "        X[1,] = np.reshape(np.flipud(gx), (*self.dim,self.n_channels)) # JLG: upside down\n",
    "        Y[1,] = np.reshape(np.flipud(fx), (*self.dim,self.n_channels))  \n",
    "        \n",
    "        '''\n",
    "        # Local GPU (use one item per mini-batch due to limited memory resources)\n",
    "        X = np.zeros((1, *self.dim, self.n_channels),dtype=np.single) \n",
    "        Y = np.zeros((1, *self.dim, self.n_channels),dtype=np.single)\n",
    "        X[0,] = np.reshape(gx, (*self.dim,self.n_channels))  \n",
    "        Y[0,] = np.reshape(fx, (*self.dim,self.n_channels))  \n",
    "        \n",
    "        '''\n",
    "        '''     \n",
    "        #Google Colaboratory GPU or local CPU (for a few epochs):\n",
    "        X = np.zeros((4, *self.dim, self.n_channels),dtype=np.single) \n",
    "        Y = np.zeros((4, *self.dim, self.n_channels),dtype=np.single)        \n",
    "        for i in range(4):\n",
    "            X[i,] = np.reshape(np.rot90(gx,i,(2,1)), (*self.dim,self.n_channels))\n",
    "            Y[i,] = np.reshape(np.rot90(fx,i,(2,1)), (*self.dim,self.n_channels))  \n",
    "        '''\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def goTrain():\n",
    "    \n",
    "    # input image dimensions\n",
    "    params = {'batch_size':1,\n",
    "            'dim':(128,128,128),\n",
    "            'n_channels':1,\n",
    "            'shuffle': True}\n",
    "    \n",
    "    seismPathT = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\train\\\\seis\\\\\"\n",
    "    faultPathT = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\train\\\\fault\\\\\"\n",
    "\n",
    "    seismPathV = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\validation\\seis\\\\\"\n",
    "    faultPathV = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\validation\\\\fault\\\\\"\n",
    "    \n",
    "    # checkpoint\n",
    "    filepath=\"D:\\\\Tesis\\\\Notebooks_JLG\\\\checkpoint-{epoch:02d}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                                 monitor=tf.keras.metrics.RootMeanSquaredError(), #'val_acc', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=False,\n",
    "                                 mode='max')\n",
    "    \n",
    "    # ORIGINAL\n",
    "    EPOCHS  = 20\n",
    "    N_train = 200\n",
    "    N_valid = 20\n",
    "    \n",
    "    # FAST TEST\n",
    "    #EPOCHS  = 10\n",
    "    #N_train = 1\n",
    "    #N_valid = 1\n",
    "    \n",
    "    \n",
    "    # Reducir el número para entrenar con un número menor de ejemplos\n",
    "    train_ID = range(N_train)\n",
    "    valid_ID = range(N_valid)\n",
    "    \n",
    "    \n",
    "    train_generator = DataGenerator(dpath=seismPathT,fpath=faultPathT,\n",
    "                                  data_IDs=train_ID,**params)\n",
    "    \n",
    "    valid_generator = DataGenerator(dpath=seismPathV,fpath=faultPathV,\n",
    "                                  data_IDs=valid_ID,**params)\n",
    "    \n",
    "    model = unet(input_size=(None, None, None,1))\n",
    "    \n",
    "#     # Así está en el original:\n",
    "#     model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "#                  loss='binary_crossentropy', \n",
    "#                  metrics=[tf.keras.metrics.RootMeanSquaredError(),\n",
    "#                          tf.keras.metrics.Accuracy()])\n",
    "\n",
    "    # PARA ANALIZAR: DICE LOSS\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "                 loss=smooth_dice_lossGPT, \n",
    "                 metrics=[tf.keras.metrics.RootMeanSquaredError(),\n",
    "                         tf.keras.metrics.BinaryAccuracy(),\n",
    "                         tf.keras.metrics.TrueNegatives(),\n",
    "                          tf.keras.metrics.TrueNegatives(),\n",
    "                          tf.keras.metrics.TruePositives(),\n",
    "                          tf.keras.metrics.FalsePositives(),\n",
    "                          tf.keras.metrics.FalseNegatives(),])\n",
    "    \n",
    "#     model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "#                  loss=SigmoidFocalCrossEntropy(from_logits = False), # unet calcula sigmoid en last layer\n",
    "#                  metrics=[tf.keras.metrics.RootMeanSquaredError(),\n",
    "#                          tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "#     # Loss escrito para entender:\n",
    "   # model.compile(optimizer = Adam(learning_rate=1e-4), \n",
    "    #               loss      = cross_entropy_balanced_JLG,\n",
    "     #              metrics   = [tf.keras.metrics.RootMeanSquaredError(),                               \n",
    "      #                          tf.keras.metrics.Accuracy()])\n",
    "\n",
    "    \n",
    "     # Utilizo el balanced loss propuesto por el autor:\n",
    "#     model.compile(optimizer=Adam(lr=1e-4), \n",
    "#                   loss = cross_entropy_balanced, # loss='binary_crossentropy'\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "# Utilizo el loss propuesto por el autor, pero escrito por mi:\n",
    "# AQUI TIENE SENTIDO QUE ACCURACY == 0 Y QUE HAYA QUE MEDIR CON RMSE\n",
    "#     model.compile(optimizer = Adam(learning_rate=1e-4), \n",
    "#                   loss      = cross_entropy_balanced_JLG,\n",
    "#                   metrics   = [tf.keras.metrics.RootMeanSquaredError(),                               \n",
    "#                                tf.keras.metrics.Accuracy(),\n",
    "#                                tf.keras.metrics.Precision(),\n",
    "#                                tf.keras.metrics.Recall()])\n",
    "\n",
    "# Utilizo el balanced loss propuesto por el autor, pero escrito por mi (con binary cross-entropy):\n",
    "#     model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "#                   loss = BinaryCrossentropyBalanced, # loss='binary_crossentropy'\n",
    "#                   metrics=['accuracy'])\n",
    " \n",
    "    \n",
    "    print(\"Data is prepared. Ready to train !\") # JLG\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(x = train_generator,\n",
    "                        validation_data=valid_generator,\n",
    "                        epochs=EPOCHS,  # few epochs to check all works\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)    \n",
    "    \n",
    "    #model.save('trained_model/fsegmentation_JLG.hdf5') # the whole model and history  \n",
    "    model.save_weights('D:\\\\Tesis\\\\Notebooks_JLG\\\\weights.h5') # just the neccesary.  \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Check model architecture:\n",
    "model_ = unet(input_size=(None, None, None,1))\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Check model's IO: \"that the model outputs something\"\n",
    "\n",
    "# training image dimensions\n",
    "n1, n2, n3 = 128, 128, 128\n",
    "\n",
    "gx  = np.random.normal(size=n1*n2*n3).reshape(n1,n2,n3)\n",
    "# convertir a tensor para el modelo:\n",
    "x   = np.reshape(gx,(1,n1,n2,n3,1))\n",
    "# predecir\n",
    "Y   = model_.predict(x,verbose=1)\n",
    "# Remuevo dimensiones para graficar\n",
    "x = x.squeeze() # (1,n1,n2,n3,1) => (n1,n2,n3)\n",
    "Y = Y.squeeze() # (1,n1,n2,n3,1) => (n1,n2,n3)\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Input (slice)\")\n",
    "plt.imshow(x[:,:,n3//2])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Output (slice)\")\n",
    "plt.imshow(Y[:,:,n3//2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training\n",
    "\n",
    "I use a few epochs to see if all works. Also, to check processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = goTrain()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy and loss over epochs:\n",
    "#showHistory(history)\n",
    "np.save('...\\\\history.npy',history.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load('...\\\\history.npy', allow_pickle=True).item() #cargar desde el directorio donde esté history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHistory(history):\n",
    "    # list all data in history\n",
    "    #print(history)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))    \n",
    "    # summarize history for rmse\n",
    "    plt.plot(history['root_mean_squared_error'],linestyle='-', marker='o', color=\"black\") # JLG\n",
    "    plt.plot(history['val_root_mean_squared_error'],linestyle='dashed', marker=(5,1), color=\"tomato\") # JLG\n",
    "    #plt.title('Error Medio Cuadrático del modelo',fontsize=20)\n",
    "    plt.ylabel('EMC',fontsize=20)\n",
    "    plt.xlabel('Época',fontsize=20)\n",
    "    #plt.legend(['Entrenamiento', 'Testeo'], loc='upper right',fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "    epochs = range(1, len(history['binary_accuracy'])+1) # assuming history is a dictionary containing the training and validation accuracy for each epoch\n",
    "    tick_positions = np.arange(0, len(epochs))\n",
    "    xtick_labels = [str(x) if i%2==0 else \"\" for i,x in enumerate(tick_positions)]\n",
    "    plt.xticks(tick_positions, xtick_labels)\n",
    "    fig.savefig(\"D:\\\\Tesis\\\\Notebooks_JLG\\\\checkpoints\\\\history_EMC.jpg\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(history['loss'],linestyle='-', marker='o',color=\"black\")\n",
    "    plt.plot(history['val_loss'],linestyle='dashed', marker=(5,1), color=\"tomato\")\n",
    "    #plt.title('Pérdida del modelo',fontsize=20)\n",
    "    plt.ylabel('Pérdida',fontsize=20)\n",
    "    plt.xlabel('Época',fontsize=20)\n",
    "    xtick_positions=np.arange(0,20)\n",
    "    xtick_labels = [str(x) if i%2==0 else \"\" for i,x in enumerate(xtick_positions)]\n",
    "    plt.xticks(xtick_positions, xtick_labels)\n",
    "    #plt.legend(['Entrenamiento', 'Testeo'], loc='upper right',fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "    fig.savefig(\"D:\\\\Tesis\\\\Notebooks_JLG\\\\checkpoints\\\\history_Loss.jpg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    fig= plt.figure(figsize=(10,6))\n",
    "    plt.plot(history['binary_accuracy'],linestyle='-', marker='o', color=\"black\") # JLG\n",
    "    plt.plot(history['val_binary_accuracy'],linestyle='dashed', marker=(5,1), color=\"tomato\") # JLG\n",
    "    #plt.title('Precisión del Modelo',fontsize=20)\n",
    "    plt.ylabel('Precisión',fontsize=20)\n",
    "    tick_positions = np.arange(0, 1.5, 0.1)\n",
    "    plt.yticks(tick_positions)\n",
    "    plt.ylim(0, 1.0)\n",
    "    xtick_positions2=np.arange(0,20)\n",
    "    xtick_labels = [str(x) if i%2==0 else \"\" for i,x in enumerate(xtick_positions)]\n",
    "    plt.xticks(xtick_positions,xtick_labels)\n",
    "    plt.xlabel('Época',fontsize=20)\n",
    "    #plt.legend(['Entrenamiento', 'Testeo'], loc='lower right',fontsize=15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "    fig.savefig(\"D:\\\\Tesis\\\\Notebooks_JLG\\\\checkpoints\\\\history_BA.jpg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "  \n",
    "    return None\n",
    "\n",
    "\n",
    "#lw=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Compute the ROC curve\n",
    "tp=history['true_positives']\n",
    "tp=np.array(tp)\n",
    "tn=history['true_negatives']\n",
    "tn=np.array(tn)\n",
    "fp=history['false_positives']\n",
    "fp=tp=np.array(fp)\n",
    "fn=history['false_negatives']\n",
    "fn=tp=np.array(fn)\n",
    "fpr=fp / (fp + tn)\n",
    "tnr=tn / (tn + fp)\n",
    "pr=history['binary_accuracy']\n",
    "    #auc = np.trapz(tpr, x=fpr)\n",
    "print(\"AUC:\", fpr, tnr)\n",
    "print(\"TP:\",tp)\n",
    "print(\"TN:\",tn)\n",
    "print(\"FP:\",fp)\n",
    "print(\"FN:\",fn)\n",
    "print(\"Precisión\", pr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "    #fig=plt.scatter(fpr, tnr)\n",
    "    #plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('True Positive Rate')\n",
    "    #plt.show()\n",
    "    \n",
    "    #Plot the Recall-Precision curve\n",
    "recall = tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"AUC:\", recall, precision)\n",
    "    #fig2=plt.scatter(recall,precision)\n",
    "    #plt.xlabel('Recall')\n",
    "    #plt.ylabel('Precision')\n",
    "F1=(2 * precision * recall) / (precision + recall)\n",
    "print(\"F1: \", F1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA METRICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import seaborn as sn\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model # to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1,n2,n3 = 128,128,128\n",
    "model = unet(input_size=(n1, n2, n3, 1))        # set architecture\n",
    "#model = Bigunet(input_size=(n1, n2, n3, 1))        # set architecture\n",
    "\n",
    "#model.load_weights(\"D:\\\\Tesis\\model\\\\pretrained_model.hdf5\") # get weights from original model from git-hub\n",
    "model.load_weights(\"...\\\\weights.h5\")# get weights from localGPU\n",
    "\n",
    "#model.load_weights(\"D:\\\\Tesis\\\\Notebooks_JLG\\\\Entrenamiento_Collab\\\\fseg_colab_dice_loss_weights.h5\")# get weights from Colab (trained on GPU, smothed dice loss function)\n",
    "\n",
    "\n",
    "#model.summary()                                   # check summary\n",
    "\n",
    "# Check prediction\n",
    "\n",
    "n1,n2,n3 = 128,128,128\n",
    "gx = np.fromfile(\"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\train\\\\seis\\\\9.dat\",dtype=np.single)#.squeeze()\n",
    "fx = np.fromfile(\"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\train\\\\fault\\\\9.dat\",dtype=np.single)#.squeeze()\n",
    "fx = tf.reshape(fx,(1,n1,n2,n3,1))\n",
    "\n",
    "# WATCH OUT FOR NORMALIZATION AND TRANPOSITION:\n",
    "gx  = Normalize(gx) # check what happens if we do not normalize the input volume ...\n",
    "gx  = np.reshape(gx,(n1,n2,n3))\n",
    "#gx  = tf.image.per_image_standardization(gx) # otra forma de normalizar (equivalente a Normalize() )\n",
    "gx  = tf.transpose(gx)\n",
    "gx  = tf.reshape(gx,(1,n1,n2,n3,1))\n",
    "Y   = model.predict(gx,verbose=1)\n",
    "Y   = tf.transpose(Y)\n",
    "gx  = tf.transpose(gx)\n",
    "\n",
    "\n",
    "\n",
    "gx = tf.squeeze(gx).numpy()\n",
    "Y  = tf.squeeze(Y).numpy()\n",
    "fx = tf.squeeze(fx).numpy()\n",
    "\n",
    "threshold = 0.5 # PLAY WITH THIS\n",
    "Y[ Y <= threshold ] = 0.  \n",
    "Y[ Y > threshold  ] = 1.\n",
    "\n",
    "#tf.shape(Y[:,:,50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra metricas para un cubo en particular\n",
    "matrix0=np.zeros((2,2))\n",
    "\n",
    "\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        matrix = confusion_matrix(fx[:,i,j], Y[:,i,j])\n",
    "        matrix0 +=matrix\n",
    "\n",
    "print(matrix0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = matrix0.ravel()\n",
    "\n",
    "\n",
    "\n",
    "fpr=fp / (fp + tn)\n",
    "print(\"False positive Rate: \", fpr)\n",
    "\n",
    "\n",
    "tnr=tn / (tn + fp)\n",
    "print(\"True Negative Rate: \", tnr)\n",
    "    \n",
    "\n",
    "\n",
    "specificity = tn/(tn+fp)\n",
    "print(\"Specificity: \", specificity)\n",
    "\n",
    "#For imbalanced learning, recall is typically used to measure the coverage of the minority class. \n",
    "#— Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Recall o Sensitivity: \", recall)\n",
    "\n",
    "\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "#In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since in order to increase the TP for the minority class, the number of FP is also often increased, resulting in reduced precision.\n",
    "#— Page 55, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "#F-Measure provides a way to combine both precision and recall into a single measure that captures both properties.\n",
    "\n",
    "F1=(2 * precision * recall) / (precision + recall)\n",
    "print(\"F1: \", F1)\n",
    "\n",
    "# Accuracy\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(Y, fx)\n",
    "print(\"Accuracy:\",m.result().numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1,k2,k3 = 50,110,60\n",
    "plt.figure(figsize=(10,10))\n",
    "#inline slice\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(np.transpose(gx[k1,:,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"True\")\n",
    "plt.imshow(np.transpose(fx[k1,:,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted\")\n",
    "plt.imshow(np.transpose(Y[k1,:,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "# xline slice\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.transpose(gx[:,k2,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.transpose(fx[:,k2,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.transpose(Y[:,k2,:]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# time-slice\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.transpose(gx[...,k3]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.transpose(fx[...,k3]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.transpose(Y[...,k3]),cmap=plt.cm.bone,interpolation='nearest',aspect=1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get validation data\n",
    "params = {'batch_size':1,'dim':(128,128,128),'n_channels':1,'shuffle': True}\n",
    "\n",
    "seismPathV = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\validation\\seis\\\\\"\n",
    "faultPathV = \"D:\\\\Tesis\\\\faultSeg-master\\\\data\\\\validation\\\\fault\\\\\"\n",
    "\n",
    "N_valid    = 10 # How many pairs to evaluate\n",
    "valid_ID   = range(N_valid)\n",
    "   \n",
    "\n",
    "validation_data = DataGenerator(dpath=seismPathV,fpath=faultPathV,\n",
    "                                data_IDs=valid_ID,**params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate trained model in `N_valid` validation data pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all data pairs in validation data:\n",
    "#model.evaluate(validation_data)\n",
    "n1,n2,n3 = 128,128,128\n",
    "model = unet(input_size=(n1, n2, n3, 1))\n",
    "model.load_weights(\"D:\\\\Tesis\\\\Notebooks_JLG\\\\weights.h5\") # get weights from localGPU\n",
    "model.compile(optimizer = Adam(learning_rate=1e-4), \n",
    "                   loss      = smooth_dice_lossGPT, \n",
    "                   metrics   = [tf.keras.metrics.RootMeanSquaredError(),tf.keras.metrics.BinaryAccuracy()])\n",
    "model.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate trained model in one mini-batch of validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate just one mini-batch\n",
    "X,Y = validation_data.__getitem__(0)\n",
    "print(tf.shape(X))\n",
    "print(tf.shape(Y))\n",
    "model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a **checkpoint** and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a checkpoint and evaluate\n",
    "model_chkp = unet(input_size=(None, None, None, 1))\n",
    "model_chkp.load_weights(\"...\\\\checkpoints\\\\fsegmentation-14.hdf5\")\n",
    "model_chkp.compile(optimizer = Adam(learning_rate=1e-4), \n",
    "                   loss      = smooth_dice_lossGPT, \n",
    "                   metrics   = [tf.keras.metrics.RootMeanSquaredError(),tf.keras.metrics.BinaryAccuracy()])\n",
    "    \n",
    "model_chkp.evaluate(validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
